{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a not very optimal way to measure the memory consumed by a file. It is not very optimal because it does not execute the file but rather gives an approximation of how much it could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process has already finished. PID still exists but it's a zombie (pid=88440)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "t_process = subprocess.Popen([\"python\", \"/Users/mema/Documents/Projects/latam_challenge/src/q2/time.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "m_process = subprocess.Popen([\"python\", \"/Users/mema/Documents/Projects/latam_challenge/src/q2/memory.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "     t_memory = psutil.Process(t_process.pid).memory_info()\n",
    "     m_memory = psutil.Process(m_process.pid).memory_info()\n",
    "     print(f\"Memory used by q1_time.py: {t_memoria.rss/1024} Kilobytes\")\n",
    "     print(f\"Memory used by q1:memory.py: {m_memoria.rss/1024} Kilobytes\")\n",
    "except psutil.NoSuchProcess as e:\n",
    "     print(f\"The process has already finished. {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to measure memory and which is the one that measures memory in each of the functionalities is using memory_usage from memory-profiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "1. It will not work if run on a notebook because '__main__' will not be found and will generate an error.\n",
    "2. partial is used to create a partial version of a function with some predefined arguments, which can be useful in certain situations where you need to reuse a function with common predefined arguments. Memory_usage is then used to measure the memory usage of that partial function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "from functools import partial\n",
    "\n",
    "def sum(n1:int, n2:int):\n",
    "    return n1 + n2\n",
    "\n",
    "mem_partial = partial(sum, n1=2, n2=3)\n",
    "mem_usage = memory_usage(mem_partial)\n",
    "\n",
    "print(f\"Memory used during the process {mem_usage[0]}, KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way execution time is measured is quite simple. We simply must initialize a timer at the beginning and another at the end of the execution and at the end the differences are subtracted. This gives us a fairly precise time in terms of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time processing: 5.00519323348999, sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def func():\n",
    "    time.sleep(5) \n",
    "        \n",
    "\n",
    "start_processing_time = time.time()\n",
    "func()\n",
    "end_processing_time = time.time()\n",
    "\n",
    "total_processing_time = end_processing_time - start_processing_time\n",
    "print(f\"Total time processing: {total_processing_time}, sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the activities use libraries such as Counter, Pandas and Dask. In addition, Dataframes from both Pandas and Dask are widely used.\n",
    "\n",
    "Counter:\n",
    "Counter is a class in the Python standard library used to count hashable objects. Provides a simple way to count the occurrence of elements in a sequence, such as a list or string.\n",
    "Typical use: Commonly used to count the frequency of elements in a list, string, or other sequence.\n",
    "\n",
    "Pandas:\n",
    "Pandas is a Python library used for data manipulation and analysis. It provides powerful and flexible data structures, such as DataFrame, that allows you to work with data in a tabular form and perform operations efficiently.\n",
    "Typical Usage: Used to load, clean, transform and analyze tabular data such as CSV, Excel, SQL, etc. data sets.\n",
    "\n",
    "Dash:\n",
    "Dask is a Python library used for parallel and distributed computing. It provides flexible data structures, such as Dask Array and Dask DataFrame, that extend the functionality of NumPy and Pandas to work with larger data sets that cannot fit in the RAM of a single machine.\n",
    "Typical Use: Used when working with large data sets that cannot be handled with Pandas due to memory limitations, or when you need to parallelize operations on large data sets to improve performance.\n",
    "Dask DataFrame vs Pandas DataFrame:\n",
    "\n",
    "Dask DataFrame: It is a distributed and parallelizable data structure that resembles a Pandas DataFrame. It allows you to work with data sets that do not fit in the RAM of a single machine by dividing them into smaller blocks and performing parallel operations on those blocks. Ideal for processing large volumes of data.\n",
    "Pandas DataFrame: It is an in-memory tabular data structure designed for data analysis. It is efficient for data sets that fit in the memory of a single machine, but may be limited for very large data sets due to memory limitations. Ideal for exploratory data analysis and in-memory data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way in which the JSON is obtained to execute is by obtaining it from a Google Storage bucket which allows us to access the information. The function load_json_from_gcs() is implemented so that it obtains the information and returns it in the form of a List of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_from_gcs() -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load JSON data from Google Cloud Storage.\n",
    "    Returns:\n",
    "        list: List of JSON objects loaded from the specified GCS blob.\n",
    "    \"\"\"\n",
    "    print(\"Loading JSON from Google Cloud Storage\")\n",
    "    storage_client = storage.Client.from_service_account_json(credentials_path)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    try:\n",
    "        content = blob.download_as_text()\n",
    "        return [orjson.loads(line) for line in content.splitlines()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to obtain the JSON information is through the local file. The load_json_from_local function is built in order to carry out exhaustive tests and in this way it does not use the Bucket to an excessive extent since it may incur expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_from_local() -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load JSON data from local file.\n",
    "    Returns:\n",
    "        list: List of JSON objects loaded from the specified local file.\n",
    "    \"\"\"\n",
    "    gcp_file = []\n",
    "    with open(json_file_local_path, 'r') as file:\n",
    "        for line in file:\n",
    "            gcp_file.append(json.loads(line))\n",
    "    return gcp_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
